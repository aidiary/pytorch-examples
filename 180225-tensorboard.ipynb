{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboardの使い方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorchでTensorboardを使うには？\n",
    "- loss, val_loss, acc, val_accをロギングする\n",
    "- グラフの可視化\n",
    "- TODO: 画像や音声を保存する方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- https://github.com/lanpa/tensorboard-pytorch\n",
    "- https://qiita.com/r9y9/items/d54162d37ec4f110f4b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardX in /Users/koichiro.mori/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: protobuf>=0.3.2 in /Users/koichiro.mori/anaconda3/lib/python3.6/site-packages (from tensorboardX)\n",
      "Requirement already satisfied: numpy in /Users/koichiro.mori/anaconda3/lib/python3.6/site-packages (from tensorboardX)\n",
      "Requirement already satisfied: six in /Users/koichiro.mori/anaconda3/lib/python3.6/site-packages (from tensorboardX)\n",
      "Requirement already satisfied: setuptools in /Users/koichiro.mori/anaconda3/lib/python3.6/site-packages (from protobuf>=0.3.2->tensorboardX)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロギングの実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 1.6287 acc: 0.6804 val_loss: 0.9516 val_acc: 0.8274\n",
      "val_acc improved from 0.00000 to 0.82740!\n",
      "epoch 1, loss: 0.7303 acc: 0.8462 val_loss: 0.5600 val_acc: 0.8709\n",
      "val_acc improved from 0.82740 to 0.87090!\n",
      "epoch 2, loss: 0.5170 acc: 0.8726 val_loss: 0.4460 val_acc: 0.8886\n",
      "val_acc improved from 0.87090 to 0.88860!\n",
      "epoch 3, loss: 0.4385 acc: 0.8853 val_loss: 0.3942 val_acc: 0.8966\n",
      "val_acc improved from 0.88860 to 0.89660!\n",
      "epoch 4, loss: 0.3973 acc: 0.8923 val_loss: 0.3629 val_acc: 0.8999\n",
      "val_acc improved from 0.89660 to 0.89990!\n",
      "epoch 5, loss: 0.3710 acc: 0.8978 val_loss: 0.3428 val_acc: 0.9050\n",
      "val_acc improved from 0.89990 to 0.90500!\n",
      "epoch 6, loss: 0.3522 acc: 0.9015 val_loss: 0.3273 val_acc: 0.9094\n",
      "val_acc improved from 0.90500 to 0.90940!\n",
      "epoch 7, loss: 0.3377 acc: 0.9058 val_loss: 0.3145 val_acc: 0.9116\n",
      "val_acc improved from 0.90940 to 0.91160!\n",
      "epoch 8, loss: 0.3259 acc: 0.9082 val_loss: 0.3050 val_acc: 0.9142\n",
      "val_acc improved from 0.91160 to 0.91420!\n",
      "epoch 9, loss: 0.3158 acc: 0.9105 val_loss: 0.2965 val_acc: 0.9181\n",
      "val_acc improved from 0.91420 to 0.91810!\n",
      "epoch 10, loss: 0.3069 acc: 0.9134 val_loss: 0.2894 val_acc: 0.9193\n",
      "val_acc improved from 0.91810 to 0.91930!\n",
      "epoch 11, loss: 0.2990 acc: 0.9153 val_loss: 0.2820 val_acc: 0.9209\n",
      "val_acc improved from 0.91930 to 0.92090!\n",
      "epoch 12, loss: 0.2916 acc: 0.9176 val_loss: 0.2771 val_acc: 0.9225\n",
      "val_acc improved from 0.92090 to 0.92250!\n",
      "epoch 13, loss: 0.2848 acc: 0.9197 val_loss: 0.2707 val_acc: 0.9239\n",
      "val_acc improved from 0.92250 to 0.92390!\n",
      "epoch 14, loss: 0.2786 acc: 0.9215 val_loss: 0.2647 val_acc: 0.9261\n",
      "val_acc improved from 0.92390 to 0.92610!\n",
      "epoch 15, loss: 0.2724 acc: 0.9234 val_loss: 0.2601 val_acc: 0.9271\n",
      "val_acc improved from 0.92610 to 0.92710!\n",
      "epoch 16, loss: 0.2666 acc: 0.9249 val_loss: 0.2541 val_acc: 0.9282\n",
      "val_acc improved from 0.92710 to 0.92820!\n",
      "epoch 17, loss: 0.2612 acc: 0.9270 val_loss: 0.2498 val_acc: 0.9307\n",
      "val_acc improved from 0.92820 to 0.93070!\n",
      "epoch 18, loss: 0.2560 acc: 0.9285 val_loss: 0.2448 val_acc: 0.9313\n",
      "val_acc improved from 0.93070 to 0.93130!\n",
      "epoch 19, loss: 0.2508 acc: 0.9299 val_loss: 0.2405 val_acc: 0.9321\n",
      "val_acc improved from 0.93130 to 0.93210!\n",
      "epoch 20, loss: 0.2460 acc: 0.9314 val_loss: 0.2367 val_acc: 0.9341\n",
      "val_acc improved from 0.93210 to 0.93410!\n",
      "epoch 21, loss: 0.2411 acc: 0.9329 val_loss: 0.2325 val_acc: 0.9342\n",
      "val_acc improved from 0.93410 to 0.93420!\n",
      "epoch 22, loss: 0.2366 acc: 0.9339 val_loss: 0.2284 val_acc: 0.9346\n",
      "val_acc improved from 0.93420 to 0.93460!\n",
      "epoch 23, loss: 0.2322 acc: 0.9355 val_loss: 0.2245 val_acc: 0.9355\n",
      "val_acc improved from 0.93460 to 0.93550!\n",
      "epoch 24, loss: 0.2278 acc: 0.9363 val_loss: 0.2214 val_acc: 0.9366\n",
      "val_acc improved from 0.93550 to 0.93660!\n",
      "epoch 25, loss: 0.2237 acc: 0.9378 val_loss: 0.2163 val_acc: 0.9381\n",
      "val_acc improved from 0.93660 to 0.93810!\n",
      "epoch 26, loss: 0.2198 acc: 0.9388 val_loss: 0.2128 val_acc: 0.9395\n",
      "val_acc improved from 0.93810 to 0.93950!\n",
      "epoch 27, loss: 0.2159 acc: 0.9402 val_loss: 0.2104 val_acc: 0.9393\n",
      "epoch 28, loss: 0.2122 acc: 0.9410 val_loss: 0.2066 val_acc: 0.9400\n",
      "val_acc improved from 0.93950 to 0.94000!\n",
      "epoch 29, loss: 0.2086 acc: 0.9424 val_loss: 0.2030 val_acc: 0.9416\n",
      "val_acc improved from 0.94000 to 0.94160!\n",
      "epoch 30, loss: 0.2050 acc: 0.9428 val_loss: 0.1998 val_acc: 0.9416\n",
      "epoch 31, loss: 0.2014 acc: 0.9440 val_loss: 0.1972 val_acc: 0.9428\n",
      "val_acc improved from 0.94160 to 0.94280!\n",
      "epoch 32, loss: 0.1980 acc: 0.9444 val_loss: 0.1941 val_acc: 0.9432\n",
      "val_acc improved from 0.94280 to 0.94320!\n",
      "epoch 33, loss: 0.1949 acc: 0.9456 val_loss: 0.1908 val_acc: 0.9447\n",
      "val_acc improved from 0.94320 to 0.94470!\n",
      "epoch 34, loss: 0.1916 acc: 0.9463 val_loss: 0.1880 val_acc: 0.9458\n",
      "val_acc improved from 0.94470 to 0.94580!\n",
      "epoch 35, loss: 0.1886 acc: 0.9470 val_loss: 0.1855 val_acc: 0.9464\n",
      "val_acc improved from 0.94580 to 0.94640!\n",
      "epoch 36, loss: 0.1856 acc: 0.9481 val_loss: 0.1833 val_acc: 0.9465\n",
      "val_acc improved from 0.94640 to 0.94650!\n",
      "epoch 37, loss: 0.1827 acc: 0.9484 val_loss: 0.1807 val_acc: 0.9472\n",
      "val_acc improved from 0.94650 to 0.94720!\n",
      "epoch 38, loss: 0.1798 acc: 0.9495 val_loss: 0.1775 val_acc: 0.9473\n",
      "val_acc improved from 0.94720 to 0.94730!\n",
      "epoch 39, loss: 0.1771 acc: 0.9501 val_loss: 0.1752 val_acc: 0.9493\n",
      "val_acc improved from 0.94730 to 0.94930!\n",
      "epoch 40, loss: 0.1744 acc: 0.9513 val_loss: 0.1723 val_acc: 0.9505\n",
      "val_acc improved from 0.94930 to 0.95050!\n",
      "epoch 41, loss: 0.1718 acc: 0.9519 val_loss: 0.1700 val_acc: 0.9504\n",
      "epoch 42, loss: 0.1692 acc: 0.9524 val_loss: 0.1681 val_acc: 0.9510\n",
      "val_acc improved from 0.95050 to 0.95100!\n",
      "epoch 43, loss: 0.1667 acc: 0.9533 val_loss: 0.1661 val_acc: 0.9510\n",
      "epoch 44, loss: 0.1643 acc: 0.9540 val_loss: 0.1637 val_acc: 0.9521\n",
      "val_acc improved from 0.95100 to 0.95210!\n",
      "epoch 45, loss: 0.1618 acc: 0.9551 val_loss: 0.1617 val_acc: 0.9523\n",
      "val_acc improved from 0.95210 to 0.95230!\n",
      "epoch 46, loss: 0.1596 acc: 0.9555 val_loss: 0.1594 val_acc: 0.9536\n",
      "val_acc improved from 0.95230 to 0.95360!\n",
      "epoch 47, loss: 0.1573 acc: 0.9562 val_loss: 0.1577 val_acc: 0.9549\n",
      "val_acc improved from 0.95360 to 0.95490!\n",
      "epoch 48, loss: 0.1552 acc: 0.9570 val_loss: 0.1555 val_acc: 0.9549\n",
      "epoch 49, loss: 0.1530 acc: 0.9573 val_loss: 0.1537 val_acc: 0.9545\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print('cuda is available!')\n",
    "\n",
    "# Hyperparameters \n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "# load data\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# model\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = MultiLayerPerceptron(784, 512, 10)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28 * 28)\n",
    "        if cuda:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.data[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def valid(test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = images.view(-1, 28 * 28)\n",
    "        if cuda:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "        images = Variable(images, volatile=True)\n",
    "        labels = Variable(labels, volatile=True)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.data[0]\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "\n",
    "    val_loss = running_loss / len(test_loader)\n",
    "    val_acc = correct / len(test_loader.dataset)\n",
    "    \n",
    "    return val_loss, val_acc\n",
    "\n",
    "# training loop\n",
    "writer = SummaryWriter()\n",
    "max_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss, acc = train(train_loader)\n",
    "    val_loss, val_acc = valid(test_loader)\n",
    "\n",
    "    # logging\n",
    "    writer.add_scalar('loss', loss, epoch)\n",
    "    writer.add_scalar('acc', acc, epoch)\n",
    "    writer.add_scalar('val_loss', val_loss, epoch)\n",
    "    writer.add_scalar('val_acc', val_acc, epoch)\n",
    "\n",
    "    print('epoch %d, loss: %.4f acc: %.4f val_loss: %.4f val_acc: %.4f'\n",
    "          % (epoch, loss, acc, val_loss, val_acc))\n",
    "\n",
    "    # save the best model\n",
    "    if val_acc > max_acc:\n",
    "        print('val_acc improved from %.5f to %.5f!' % (max_acc, val_acc))\n",
    "        max_acc = val_acc\n",
    "        model_file = 'epoch%03d-%.4f-%.4f.pth' % (epoch, val_loss, val_acc)\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "\n",
    "writer.export_scalars_to_json('all_scalars.json')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/koichiro.mori/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "TensorBoard 1.5.1 at http://o-05349-mac.local:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
