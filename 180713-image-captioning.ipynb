{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    "        \n",
    "        if (i + 1) % 100000 == 0:\n",
    "            print('[{}/{}] Tokenized the captions.'.format(i + 1, len(ids)))\n",
    "    \n",
    "    # 単語頻度がthreshold以上の単語のみ使う\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "    \n",
    "    # ボキャブラリを作成\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.73s)\n",
      "creating index...\n",
      "index created!\n",
      "[100000/414113] Tokenized the captions.\n",
      "[200000/414113] Tokenized the captions.\n",
      "[300000/414113] Tokenized the captions.\n",
      "[400000/414113] Tokenized the captions.\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(json='data/annotations/captions_train2014.json', threshold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 9957\n"
     ]
    }
   ],
   "source": [
    "print('Total vocabulary size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, size):\n",
    "    \"\"\"Resize an image to the given size.\"\"\"\n",
    "    return image.resize(size, Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(image_dir, output_dir, size):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    images = os.listdir(image_dir)\n",
    "    num_images = len(images)\n",
    "    for i, image in enumerate(images):\n",
    "        with open(os.path.join(image_dir, image), 'rb') as f:\n",
    "            with Image.open(f) as img:\n",
    "                img = resize_image(img, size)\n",
    "                img.save(os.path.join(output_dir, image), img.format)\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print('[{}/{}] Resized the images and saved into {}.'.format(i + 1, num_images, output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_images('./data/train2014', './data/resized2014', (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        # 最後のFC層を削除\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet152(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "    \n",
    "    def forward(self, features, captions, length):\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, length, batch_first=True)\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, features, stats=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(data.Dataset):\n",
    "    def __init__(self, root, json, vocab, transform=None):\n",
    "        self.root = root\n",
    "        self.coco = COCO(json)\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        coco = self.coco\n",
    "        vocab = self.vocab\n",
    "        # アノテーションのID\n",
    "        ann_id = self.ids[index]\n",
    "        # キャプションと画像IDを取り出す\n",
    "        caption = coco.anns[ann_id]['caption']\n",
    "        img_id = coco.anns[ann_id]['image_id']\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        \n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # キャプションを単語IDに変換\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.66s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = CocoDataset(root='./data/resized2014',\n",
    "                   json='./data/annotations/captions_train2014.json',\n",
    "                   vocab=vocab,\n",
    "                   transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=RGB size=256x256 at 0x1A27B92B38>, tensor([  1.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,   2.]))\n",
      "(<PIL.Image.Image image mode=RGB size=256x256 at 0x1A27B92C50>, tensor([  1.,   4.,  12.,  13.,  14.,   4.,  15.,   7.,  16.,  14.,\n",
      "         17.,  18.,  19.,   2.]))\n"
     ]
    }
   ],
   "source": [
    "# 画像のサイズは一緒だが、キャプションの長さが異なるので普通にDataLoaderでバッチ化はできない\n",
    "# collate_fn()を追加してバッチサイズを揃える\n",
    "print(coco[0])\n",
    "print(coco[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    # dataは (image, caption) のリスト（ミニバッチサイズ）\n",
    "    # image: (3, 256, 256)\n",
    "    # caption: (?) サイズは任意\n",
    "    # captionの長さが長い順にソート\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # 画像をマージ\n",
    "    # imageのtupleを4Dテンソルに変換\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # キャプションをマージ\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    # ミニバッチ内の最長のキャプションに合わせる\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "\n",
    "    return images, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(root, json, vocab, transform, batch_size, shuffle, num_workers):\n",
    "    coco = CocoDataset(root, json, vocab, transform)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=coco,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.70s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "data_loader = get_loader(root='./data/resized2014',\n",
    "                         json='./data/annotations/captions_train2014.json',\n",
    "                         vocab=vocab,\n",
    "                         transform=transform,\n",
    "                         batch_size=16,\n",
    "                         shuffle=True,\n",
    "                         num_workers=0)\n",
    "\n",
    "images, targets, lengths = iter(loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 25])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 17, 15, 15, 15, 14, 14, 13, 13, 13, 13, 12, 12, 11, 11, 10]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(256).to(device)\n",
    "decoder = DecoderRNN(256, 512, len(vocab), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(9957, 256)\n",
       "  (lstm): LSTM(256, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=9957, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25883"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_step = len(data_loader)\n",
    "total_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n",
      "torch.Size([16, 20])\n",
      "[20, 17, 17, 16, 16, 15, 14, 13, 13, 13, 13, 12, 12, 12, 12, 11]\n"
     ]
    }
   ],
   "source": [
    "images, captions, lengths = iter(data_loader).next()\n",
    "print(images.size())\n",
    "print(captions.size())\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = encoder(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: torch.Size([16, 256])\n",
      "captions: torch.Size([16, 20])\n",
      "length: [20, 17, 17, 16, 16, 15, 14, 13, 13, 13, 13, 12, 12, 12, 12, 11]\n",
      "embeddings: torch.Size([16, 20, 256])\n",
      "features.unsqueeze: torch.Size([16, 1, 256])\n",
      "embeddings: torch.Size([16, 21, 256])\n",
      "hiddens[0]: torch.Size([226, 512])\n",
      "hiddens[1]: torch.Size([20])\n",
      "outputs: torch.Size([226, 9957])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.4075e-02, -5.2083e-02,  2.3038e-02,  ...,  1.3599e-02,\n",
       "          2.1245e-02,  4.8677e-02],\n",
       "        [-1.6939e-02,  5.6714e-02, -4.4613e-02,  ...,  2.1167e-02,\n",
       "          6.9803e-02, -2.8775e-02],\n",
       "        [-5.7114e-02, -6.7729e-02, -1.5773e-02,  ...,  2.5258e-02,\n",
       "          1.9828e-02,  2.3223e-03],\n",
       "        ...,\n",
       "        [ 2.2226e-02, -1.0118e-01,  2.9702e-02,  ..., -6.6260e-02,\n",
       "          1.2401e-02,  8.5465e-02],\n",
       "        [-1.1013e-01, -3.8904e-02,  8.6993e-02,  ...,  9.1666e-03,\n",
       "          7.5423e-02,  3.4090e-02],\n",
       "        [ 4.7738e-02, -1.1129e-01,  3.7872e-02,  ...,  3.2355e-02,\n",
       "         -7.0128e-02, -5.0746e-02]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "    \n",
    "    def forward(self, features, captions, length):\n",
    "        # featuresは単語のEmbeddingsと同じサイズ\n",
    "        # lengthsは各バッチの系列長\n",
    "        # \n",
    "        print('features:', features.size())\n",
    "        print('captions:', captions.size())\n",
    "        print('length:', length)\n",
    "        embeddings = self.embed(captions)\n",
    "        print('embeddings:', embeddings.size())\n",
    "        print('features.unsqueeze:', features.unsqueeze(1).size())\n",
    "        # 画像のfeaturesを系列の最初に追加する\n",
    "        # 状態ではなく、入力となる単語系列と同じ扱い\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        print('embeddings:', embeddings.size())\n",
    "        # 可変長のミニバッチを使う時はpackが必要\n",
    "        # lengthに各バッチの要素の本当の系列長が入っている\n",
    "        packed = pack_padded_sequence(embeddings, length, batch_first=True)\n",
    "        # hとcは入れないので0で初期化される\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        print('hiddens[0]:', hiddens[0].size())\n",
    "        print('hiddens[1]:', hiddens[1].size())\n",
    "        # hiddens[0]はバッチを構成する全ての系列の各時刻でのLSTMの出力\n",
    "        # hiddens[1]は何？\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        # outputsのサイズは\n",
    "        # [全てのバッチの全ての系列長の合計 sum(lengths), vocab]\n",
    "        print('outputs:', outputs.size())\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, features, stats=None):\n",
    "        pass\n",
    "\n",
    "decoder = DecoderRNN(256, 512, len(vocab), 1)\n",
    "decoder(features, captions, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     4,    51,\n",
       "           33,     4,    33,     4,     4,    48,     4,    51,    92,\n",
       "           33,   251,    51,     4,   251,   268,   455,  1593,    61,\n",
       "         2516,   331,    93,   561,   254,   345,   508,  2894,    53,\n",
       "           52,   170,   131,    81,   925,   377,     7,    65,   429,\n",
       "          131,   116,   170,  8366,   768,  1262,   367,   286,    78,\n",
       "            4,    14,    72,   161,    21,  4221,  2436,   286,   325,\n",
       "           22,     4,   820,   112,   616,   112,     4,   372,  2420,\n",
       "            4,  1460,   372,   162,   102,     4,    40,     4,   270,\n",
       "          271,    33,   112,    33,   532,   225,   792,    27,   162,\n",
       "          287,    33,     4,    21,     4,    20,    22,     4,   258,\n",
       "           33,   110,  2795,    40,     7,   111,  2313,   436,   229,\n",
       "         2167,   408,   170,   104,    33,  1294,    14,   427,    78,\n",
       "            4,     4,  1021,    14,     4,    33,   166,    14,    78,\n",
       "           78,   612,   439,   278,     4,    78,     4,  4728,    85,\n",
       "         3772,   718,   170,   502,     3,     3,     4,     4,    33,\n",
       "          566,   418,   514,   146,   731,  9221,   133,   231,   131,\n",
       "          544,   162,    78,    78,   279,   326,    34,   718,    33,\n",
       "         1014,    15,    19,    19,     2,   295,   361,    40,   379,\n",
       "            4,     4,  3783,    19,    19,    19,   272,     2,     2,\n",
       "            2,     2,    40,     4,    47,     4,   210,  1139,    19,\n",
       "            2,     2,     2,     2,     4,  1493,   409,   357,   211,\n",
       "           19,     2,   230,  3636,   527,    19,    19,     2,    22,\n",
       "           19,    19,     2,     2,  8124,     2,     2,   633,    19,\n",
       "            2])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各ミニバッチのcaptionsが順番に出てくる（paddingした0は無視される）\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     4,   268,    81,    14,  2420,   792,     7,  1021,\n",
       "          3772,   231,   295,    40,     4,   230,    22,  8124,   633,\n",
       "            19,     2],\n",
       "        [    1,    51,   455,   925,    72,     4,    27,   111,    14,\n",
       "           718,   131,   361,     4,  1493,  3636,    19,     2,     0,\n",
       "             0,     0],\n",
       "        [    1,    33,  1593,   377,   161,  1460,   162,  2313,     4,\n",
       "           170,   544,    40,    47,   409,   527,    19,     2,     0,\n",
       "             0,     0],\n",
       "        [    1,     4,    61,     7,    21,   372,   287,   436,    33,\n",
       "           502,   162,   379,     4,   357,    19,     2,     0,     0,\n",
       "             0,     0],\n",
       "        [    1,    33,  2516,    65,  4221,   162,    33,   229,   166,\n",
       "             3,    78,     4,   210,   211,    19,     2,     0,     0,\n",
       "             0,     0],\n",
       "        [    1,     4,   331,   429,  2436,   102,     4,  2167,    14,\n",
       "             3,    78,     4,  1139,    19,     2,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    1,     4,    93,   131,   286,     4,    21,   408,    78,\n",
       "             4,   279,  3783,    19,     2,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    1,    48,   561,   116,   325,    40,     4,   170,    78,\n",
       "             4,   326,    19,     2,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    1,     4,   254,   170,    22,     4,    20,   104,   612,\n",
       "            33,    34,    19,     2,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    1,    51,   345,  8366,     4,   270,    22,    33,   439,\n",
       "           566,   718,    19,     2,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    1,    92,   508,   768,   820,   271,     4,  1294,   278,\n",
       "           418,    33,   272,     2,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    1,    33,  2894,  1262,   112,    33,   258,    14,     4,\n",
       "           514,  1014,     2,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    1,   251,    53,   367,   616,   112,    33,   427,    78,\n",
       "           146,    15,     2,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    1,    51,    52,   286,   112,    33,   110,    78,     4,\n",
       "           731,    19,     2,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    1,     4,   170,    78,     4,   532,  2795,     4,  4728,\n",
       "          9221,    19,     2,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    1,   251,   131,     4,   372,   225,    40,     4,    85,\n",
       "           133,     2,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: torch.Size([16, 256])\n",
      "captions: torch.Size([16, 20])\n",
      "length: [20, 18, 14, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11]\n",
      "embeddings: torch.Size([16, 20, 256])\n",
      "features.unsqueeze: torch.Size([16, 1, 256])\n",
      "embeddings: torch.Size([16, 21, 256])\n",
      "hiddens[0]: torch.Size([206, 512])\n",
      "hiddens[1]: torch.Size([20])\n",
      "outputs: torch.Size([206, 9957])\n",
      "Epoch [0/5], Step[0/25883], Loss: 9.2195, Perplexity: 10092.2824\n"
     ]
    }
   ],
   "source": [
    "total_step = len(data_loader)\n",
    "for epoch in range(5):\n",
    "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        # ミニバッチは可変長なのでlengthsで指定された長さで各系列の出力単語（正解）をpackする\n",
    "        # targetsの長さは系列長の合計と等しい\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "        \n",
    "        # forward\n",
    "        features = encoder(images)\n",
    "        # [ミニバッチの系列長合計, vocab]\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                  .format(epoch, 5, i, total_step, loss.item(), np.exp(loss.item())))\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
