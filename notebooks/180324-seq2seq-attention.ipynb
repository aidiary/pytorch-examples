{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation with a Sequence to Sequence Network and Attention\n",
    "\n",
    "- http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- フランス語を英語に翻訳するタスク\n",
    "- http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 各単語を one-hot-encoding で表す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0  # Start of Sentence\n",
    "EOS_token = 1  # End of Sentence\n",
    "\n",
    "class Lang:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hello': 2, 'world': 3, 'I': 4, 'am': 5, 'a': 6, 'student': 7, 'dog': 8}\n",
      "{0: 'SOS', 1: 'EOS', 2: 'Hello', 3: 'world', 4: 'I', 5: 'am', 6: 'a', 7: 'student', 8: 'dog'}\n",
      "{'Hello': 1, 'world': 1, 'I': 2, 'am': 2, 'a': 2, 'student': 1, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "english = Lang('english')\n",
    "english.addSentence('Hello world')\n",
    "english.addSentence('I am a student')\n",
    "english.addSentence('I am a dog')\n",
    "\n",
    "print(english.word2index)\n",
    "print(english.index2word)\n",
    "print(english.word2count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UnicodeをAsciiに変換\n",
    "- .!?以外の句読点を削除\n",
    "- アルファベット以外の文字は空白に\n",
    "- 小文字に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r'([.!?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' slusarski . hello world !'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizeString('ああ Ślusàrski. hello, world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- この段階ではLangオブジェクトを作るだけで空のまま"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = readLangs('eng', 'fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135842"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['go .', 'va !'],\n",
       " ['run !', 'cours !'],\n",
       " ['run !', 'courez !'],\n",
       " ['wow !', 'ca alors !'],\n",
       " ['fire !', 'au feu !'],\n",
       " ['help !', 'a l aide !'],\n",
       " ['jump .', 'saute .'],\n",
       " ['stop !', 'ca suffit !'],\n",
       " ['stop !', 'stop !'],\n",
       " ['stop !', 'arrete toi !']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 長さが10単語以内で\n",
    "- I am, He is ... という文章に絞り込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "# 対象とする文章か判定\n",
    "# p[0]にfra、p[1]にengが来ることが前提\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "# 対象とする文章に絞り込む\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- eng-fra.txtというファイルしかないのでlang1=eng、lang2=fraとするしかない\n",
    "- fra => engとしたいのでreverse=Trueで登録しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10853 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4489\n",
      "eng 2925\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je ne suis pas ton ami .', 'i m no friend of yours .']\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `forward()` は文の単語シーケンスの1つの単語のみを入れる実装になっている\n",
    "- 文は外側でforループを回す\n",
    "- 文を単語のシーケンスとしてforwardにまとめて入れる実装にしてもよい？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def variablesFromPair(pair):\n",
    "    input_variable = variableFromSentence(input_lang, pair[0])\n",
    "    target_variable = variableFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input, prev_hidden):\n",
    "        # inputのシーケンス長は1なので文をまるごとではなく単語を1つだけ入れる\n",
    "        # (seq_len, batch, input_size)\n",
    "#         print('input:', input.size())\n",
    "#         print('prev_hidden:', prev_hidden.size())\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "#         print('embedded:', embedded.size())\n",
    "        output = embedded\n",
    "        # outputはシーケンスの各要素を入れたときの出力がすべて保存される\n",
    "        # hiddenはシーケンスの最後の要素を入れたあとの状態\n",
    "        output, hidden = self.gru(output, prev_hidden)\n",
    "#         print('output:', output.size())\n",
    "#         print('hidden:', hidden.size())\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input, prev_hidden):\n",
    "#         print('input:', input.size())\n",
    "#         print('prev_hidden:', prev_hidden.size())\n",
    "\n",
    "        # (seq_len, batch, input_size)\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "#         print('embedded:', output.size())\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, prev_hidden)\n",
    "#         print('output:', output.size())\n",
    "#         print('hidden:', hidden.size())\n",
    "        # シーケンス長は1なので[0]のみ\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "#         print('softmax:', output.size())\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "torch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "# ENCODER TEST\n",
    "hidden_size = 256\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "\n",
    "# データ\n",
    "train_pair = variablesFromPair(random.choice(pairs))\n",
    "input = train_pair[0]   # french word_id list\n",
    "target = train_pair[1]  # english word_id list\n",
    "\n",
    "input_length = input.size()[0]\n",
    "target_length = target.size()[0]\n",
    "\n",
    "print(input_length, target_length)\n",
    "\n",
    "# 初期状態\n",
    "encoder_hidden = encoder.initHidden()\n",
    "print(encoder_hidden.size())\n",
    "\n",
    "# 系列の要素を1つずつ入力する\n",
    "# 隠れ状態は上書きしていく\n",
    "\n",
    "# encoderの出力はAttention Decoderで使うので保存しておく\n",
    "encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "\n",
    "for i in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(input[i], encoder_hidden)\n",
    "    encoder_outputs[i] = encoder_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1])\n",
      "prev_hidden: torch.Size([1, 1, 256])\n",
      "embedded: torch.Size([1, 1, 256])\n",
      "output: torch.Size([1, 1, 256])\n",
      "hidden: torch.Size([1, 1, 256])\n",
      "softmax: torch.Size([1, 2925])\n",
      "input: torch.Size([1])\n",
      "prev_hidden: torch.Size([1, 1, 256])\n",
      "embedded: torch.Size([1, 1, 256])\n",
      "output: torch.Size([1, 1, 256])\n",
      "hidden: torch.Size([1, 1, 256])\n",
      "softmax: torch.Size([1, 2925])\n",
      "input: torch.Size([1])\n",
      "prev_hidden: torch.Size([1, 1, 256])\n",
      "embedded: torch.Size([1, 1, 256])\n",
      "output: torch.Size([1, 1, 256])\n",
      "hidden: torch.Size([1, 1, 256])\n",
      "softmax: torch.Size([1, 2925])\n",
      "input: torch.Size([1])\n",
      "prev_hidden: torch.Size([1, 1, 256])\n",
      "embedded: torch.Size([1, 1, 256])\n",
      "output: torch.Size([1, 1, 256])\n",
      "hidden: torch.Size([1, 1, 256])\n",
      "softmax: torch.Size([1, 2925])\n",
      "input: torch.Size([1])\n",
      "prev_hidden: torch.Size([1, 1, 256])\n",
      "embedded: torch.Size([1, 1, 256])\n",
      "output: torch.Size([1, 1, 256])\n",
      "hidden: torch.Size([1, 1, 256])\n",
      "softmax: torch.Size([1, 2925])\n"
     ]
    }
   ],
   "source": [
    "# DECODER TEST\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words)\n",
    "\n",
    "# Decoderの入力初期値\n",
    "decoder_input = Variable(torch.LongTensor([SOS_token]))\n",
    "# Decoderの隠れ状態初期値はEncoderの隠れ状態\n",
    "decoder_hidden = encoder_hidden\n",
    "\n",
    "for i in range(target_length):\n",
    "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "    # 次の正解を入れる\n",
    "    decoder_input = target[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "\n",
    "        # attetnionの重みを求める写像\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        \n",
    "        # decoder入力とcontextをまとめる写像？\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, prev_hidden, encoder_outputs):\n",
    "#         print('input:', input.size())\n",
    "#         print('prev_hidden:', prev_hidden.size())\n",
    "#         print('encoder_outputs:', encoder_outputs.size())\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "#         print('embedded:', embedded.size())\n",
    "\n",
    "        # catするのはprev_hiddenとDecoderへの入力 embedded であっている？\n",
    "        # 論文の式(6)を見るとencoder_outputsな気がする\n",
    "        # atten_weights = alpha\n",
    "#         print('before:', torch.cat((embedded[0], prev_hidden[0]), 1).size())\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], prev_hidden[0]), 1)), dim=1)\n",
    " \n",
    "#         print('attn_weights:', attn_weights.size())\n",
    "\n",
    "        # encoderの出力をattentionで重み付け\n",
    "        # 式(5) attn_applied = ci\n",
    "        # encoder_outputs = hj\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "#         print(attn_weights.unsqueeze(0).size())\n",
    "#         print(encoder_outputs.unsqueeze(0).size())\n",
    "#         print('attn_applied:', attn_applied.size())\n",
    "\n",
    "        # embedded[0] = y_{i-1}\n",
    "        # (1) decoderへの入力 と (3) attentionされたcontextをまとめる写像\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        # outputがattentionで重み付けされたcontext vector？\n",
    "        # gruのhiddenの方に入れると思ってたけどoutputに入れている\n",
    "        # prev_hiddenは別に入れる必要があるのでoutputに入れている\n",
    "        # (1) decoderへの入力 (2) decoderのprev_hidden (3) attentionされたcontextを入れる必要ある\n",
    "        # 3つは入れられないので (1) と (3) をまとめてinputしている？\n",
    "        output, hidden = self.gru(output, prev_hidden)\n",
    "        \n",
    "#         print('output:', output.size())\n",
    "#         print('hidden:', hidden.size())\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "#         print(output.size())\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1])\n",
      "prev_hidden: torch.Size([1, 1, 256])\n",
      "encoder_outputs: torch.Size([10, 256])\n",
      "embedded: torch.Size([1, 1, 256])\n",
      "before: torch.Size([1, 512])\n",
      "attn_weights: torch.Size([1, 10])\n",
      "torch.Size([1, 1, 10])\n",
      "torch.Size([1, 10, 256])\n",
      "attn_applied: torch.Size([1, 1, 256])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "output: torch.Size([1, 1, 256])\n",
      "hidden: torch.Size([1, 1, 256])\n",
      "torch.Size([1, 2925])\n"
     ]
    }
   ],
   "source": [
    "# ATTENTION DECODER TEST\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, max_length=MAX_LENGTH)\n",
    "\n",
    "# Decoderの入力初期値（SOSから始める）\n",
    "decoder_input = Variable(torch.LongTensor([SOS_token]))\n",
    "\n",
    "# Decoderの隠れ状態初期値はEncoderの隠れ状態\n",
    "decoder_hidden = encoder_hidden\n",
    "\n",
    "for i in range(target_length):\n",
    "    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    decoder_input = target[i]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \"\"\"1つの系列長対から学習\"\"\"\n",
    "    # encoderの隠れ状態を初期化\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # 入力の系列長\n",
    "    input_length = input_variable.size()[0]\n",
    "    # 出力の系列長\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # attentionではencoderの出力が必要なので保存しておく\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    if use_cuda:\n",
    "        encoder_outputs.cuda()\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # 系列の各要素をforループで処理していく\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]  # 3D tensorではなく256次元ベクトルのみ保存\n",
    "\n",
    "    # decoderへの最初の入力は <SOS> から始める\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    if use_cuda:\n",
    "        decoder_input.cuda()\n",
    "\n",
    "    # encoderの最後の出力をdecoderの初期状態とする\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # 50%の確率でDecoderに教師信号を入力し、50%の確率で1つ前に生成した出力を入力する\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: 次の入力としてターゲット（正解の教師）を入力する\n",
    "        for di in range(target_length):\n",
    "            # Attentionを使うのでencoder_outputsを入力とする\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            # 次のdecoderへの入力として正解を入れる\n",
    "            decoder_input = target_variable[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            \n",
    "            # 次のdecoderへの入力は今回の出力を入れる\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]  # 分類された単語のインデックス\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            if use_cuda:\n",
    "                decoder_input.cuda()\n",
    "    \n",
    "            # 系列終了の出力が出たらそこで終わり\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    # optimizers\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # training data\n",
    "    # iterationの数だけランダムに (french, english) の文章ペアデータを生成する\n",
    "    training_pairs = [variablesFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    # 訓練データの (french, english) の系列対を学習\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]   # french word_id list\n",
    "        target_variable = training_pair[1]  # english word_id list\n",
    "        \n",
    "        # 1つの系列対から学習\n",
    "        loss = train(input_variable, target_variable,\n",
    "                     encoder, decoder,\n",
    "                     encoder_optimizer, decoder_optimizer,\n",
    "                     criterion)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%d %d%% %.4f' % (iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "        \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
