{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "- https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "\n",
    "- http://cympfh.cc/paper/VAE.html\n",
    "- https://jaan.io/what-is-variational-autoencoder-vae-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print('cuda is available!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data',\n",
    "                   train=False,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)  # mu\n",
    "        self.fc22 = nn.Linear(400, 20)  # logvar\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "#             print('std:', std.size())\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "#             print('eps:', eps.size())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        mu, logvar = self.encode(x)\n",
    "#         print('mu:', mu.size())\n",
    "#         print('logvar:', logvar.size())\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "#         print('z:', z.size())\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `exp_()` でなくて `exp()` でも同じでは？\n",
    "- eps（N(0,1) の乱数）がベクトルになっている\n",
    "- 潜在変数の各次元ごとに異なる乱数をかけることになる\n",
    "- 数式見るとスカラーみたいだけどこの実装でよいのか？\n",
    "- `eps = Variable(std.data.new(1).normal_())` ではダメ？\n",
    "- $logvar = \\log \\Sigma^2$\n",
    "- $std = \\exp ( 0.5 \\log \\Sigma^2) = \\exp (\\log \\Sigma) = \\Sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "data, _ = iter(train_loader).next()\n",
    "print(data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784])\n",
      "torch.Size([128, 20])\n",
      "torch.Size([128, 20])\n"
     ]
    }
   ],
   "source": [
    "outputs, mu, logvar = model(Variable(data))\n",
    "print(outputs.size())\n",
    "print(mu.size())\n",
    "print(logvar.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # size_average=Falseなのでバッチ内のサンプルの合計loss\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average=False)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(data),\n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)\n",
    "            ))\n",
    "    \n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch,\n",
    "        train_loss / len(train_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VAEは教師なし学習なのでラベルは使わない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.498535\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 304.871979\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 246.235764\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 227.771255\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 211.217621\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 214.487366\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 204.445663\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 203.452652\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 194.351486\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 193.790466\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 175.084488\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 170.913773\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 169.013672\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 166.865723\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 163.366638\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 154.371277\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 147.533264\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 147.393219\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 159.925705\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 151.102158\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 149.193542\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 148.485489\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 147.492767\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 148.093933\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 146.659683\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 147.838760\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 142.969299\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 137.502014\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 143.477814\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 143.771973\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 134.186096\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 140.159088\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 136.499023\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 132.744614\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 131.540253\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 127.106461\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 134.473297\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 134.332123\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 134.702072\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 131.858078\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 132.684616\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 134.741913\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 127.206802\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 135.346466\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 130.030411\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 128.304031\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 130.246918\n",
      "====> Epoch: 1 Average loss: 162.9688\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 129.170242\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 132.258652\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 119.198776\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 130.470367\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 123.445618\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 124.652992\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 129.924118\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 120.802994\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 124.504539\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 125.464615\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 122.632111\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 126.759293\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 119.833374\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 121.579666\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 129.488144\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 123.435440\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 123.765327\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 122.067360\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 116.531944\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 124.312813\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 121.193665\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 120.533119\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 116.378098\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 119.690880\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 120.940193\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 117.431450\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 117.728516\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 121.082436\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 118.790176\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 115.178444\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 119.566833\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 118.416969\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 118.577972\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 120.298813\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 117.102303\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 120.820389\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 121.446495\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 112.404129\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 122.189827\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 118.294518\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 119.019768\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 119.118484\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 112.748367\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 114.922394\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 116.295914\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 109.857193\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 115.291626\n",
      "====> Epoch: 2 Average loss: 120.6041\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 113.342758\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 121.099510\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 115.786247\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 113.629967\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 119.865402\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 112.644081\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 117.949005\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 112.742996\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 115.443253\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 115.889923\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 118.977577\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 111.976334\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 111.179901\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 117.695145\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 111.632050\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 114.728821\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 111.823929\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 116.876587\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-12f5880268ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-721817feba06>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
