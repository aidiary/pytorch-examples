{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = 'cornell movie-dialogs corpus'\n",
    "corpus = os.path.join('data', corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
      "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
      "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
      "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
      "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
      "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
      "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
      "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
      "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
      "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
     ]
    }
   ],
   "source": [
    "printLines(os.path.join(corpus, 'movie_lines.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLines(fileName, fields):\n",
    "    lines = {}\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' +++$+++ ')\n",
    "            lineObj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                lineObj[field] = values[i]\n",
    "            lines[lineObj['lineID']] = lineObj\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "lines = loadLines(os.path.join(corpus, 'movie_lines.txt'), MOVIE_LINES_FIELDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lineID': 'L1045',\n",
       " 'characterID': 'u0',\n",
       " 'movieID': 'm0',\n",
       " 'character': 'BIANCA',\n",
       " 'text': 'They do not!\\n'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines['L1045']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadConversations(fileName, lines, fields):\n",
    "    conversations = []\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' +++$+++ ')\n",
    "            convObj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                convObj[field] = values[i]\n",
    "            lineIds = eval(convObj['utteranceIDs'])\n",
    "            convObj['lines'] = []\n",
    "            for lineId in lineIds:\n",
    "                convObj['lines'].append(lines[lineId])\n",
    "            conversations.append(convObj)\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = loadConversations(os.path.join(corpus, 'movie_conversations.txt'),\n",
    "                                  lines, MOVIE_CONVERSATIONS_FIELDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'character1ID': 'u0',\n",
       " 'character2ID': 'u2',\n",
       " 'movieID': 'm0',\n",
       " 'utteranceIDs': \"['L194', 'L195', 'L196', 'L197']\\n\",\n",
       " 'lines': [{'lineID': 'L194',\n",
       "   'characterID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'BIANCA',\n",
       "   'text': 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\n'},\n",
       "  {'lineID': 'L195',\n",
       "   'characterID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'CAMERON',\n",
       "   'text': \"Well, I thought we'd start with pronunciation, if that's okay with you.\\n\"},\n",
       "  {'lineID': 'L196',\n",
       "   'characterID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'BIANCA',\n",
       "   'text': 'Not the hacking and gagging and spitting part.  Please.\\n'},\n",
       "  {'lineID': 'L197',\n",
       "   'characterID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'CAMERON',\n",
       "   'text': \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSentencePairs(conversations):\n",
    "    qa_pairs = []\n",
    "    for conversation in conversations:\n",
    "        for i in range(len(conversation['lines']) - 1):\n",
    "            inputLine = conversation['lines'][i]['text'].strip()\n",
    "            targetLine = conversation['lines'][i + 1]['text'].strip()\n",
    "            if inputLine and targetLine:\n",
    "                qa_pairs.append([inputLine, targetLine])\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = os.path.join(corpus, 'formatted_movie_lines.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = '\\t'\n",
    "delimiter = str(codecs.decode(delimiter, 'unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter)\n",
    "    for pair in extractSentencePairs(conversations):\n",
    "        writer.writerow(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = extractSentencePairs(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\r\\n\"\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\r\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\r\\n\"\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\r\\n\"\n",
      "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\r\\n\"\n",
      "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\r\\n\"\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\r\\n'\n",
      "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\r\\n\"\n",
      "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "printLines(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and trim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voc:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: 'PAD', SOS_token: 'SOS', EOS_token: 'EOS'}\n",
    "        self.num_words = 3\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "        \n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index),\n",
    "            len(keep_words) / len(self.word2index)))\n",
    "        \n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readVocs(datafile, corpus_name):\n",
    "    print('Reading lines...')\n",
    "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
    "    print('Start preparing training data ...')\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print('Read {!s} sentence pairs'.format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print('Trimmed to {!s} sentence pairs'.format(len(pairs)))\n",
    "    print('Counting words ...')\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print('Counted words:', voc.num_words)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join('data', 'save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 221282 sentence pairs\n",
      "Trimmed to 64271 sentence pairs\n",
      "Counting words ...\n",
      "Counted words: 18008\n"
     ]
    }
   ],
   "source": [
    "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 3\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    voc.trim(MIN_COUNT)\n",
    "    \n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "        \n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "    \n",
    "    print('Trimmed from {} pairs to {}, {:.4f} of total'.format(\n",
    "        len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)\n",
    "    ))\n",
    "    \n",
    "    return keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 7823 / 18005 = 0.4345\n",
      "Trimmed from 64271 pairs to 53165, 0.8272 of total\n"
     ]
    }
   ],
   "source": [
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"hi\", \"good morning sir\", \"thank you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16, 2], [51, 665, 572, 2], [383, 7, 2]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "indexes_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 51, 383), (2, 665, 7), (0, 572, 2), (0, 2, 0)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padList = zeroPadding(indexes_batch)\n",
    "padList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 51, 383), (2, 665, 7), (None, 572, 2), (None, 2, None)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.zip_longest(*indexes_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1], [1, 1, 1], [0, 1, 1], [0, 1, 0]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = binaryMatrix(padList)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 1, 1],\n",
       "        [0, 1, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ByteTensor(mask)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(' ')), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['oh boy ! a jacket !', 'your mom made that all by herself .'],\n",
       " ['yes .', 'nobody . accounting .'],\n",
       " ['what s the matter with you ?', 'toothache .'],\n",
       " ['what are you working on ?', 'just a little experiment .'],\n",
       " ['feel okay ?', 'yeah .']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = [random.choice(pairs) for _ in range(small_batch_size)]\n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what s the matter with you ?', 'toothache .'],\n",
       " ['oh boy ! a jacket !', 'your mom made that all by herself .'],\n",
       " ['what are you working on ?', 'just a little experiment .'],\n",
       " ['feel okay ?', 'yeah .'],\n",
       " ['yes .', 'nobody . accounting .']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.sort(key=lambda x: len(x[0].split(' ')), reverse=True)\n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = batch2TrainData(voc, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  50,  124,   50, 1026,  318],\n",
       "         [  37,  519,   92,   62,    4],\n",
       "         [  53,   66,    7,    6,    2],\n",
       "         [ 341,   12,  937,    2,    0],\n",
       "         [ 169, 2468,  177,    0,    0],\n",
       "         [   7,   66,    6,    0,    0],\n",
       "         [   6,    2,    2,    0,    0],\n",
       "         [   2,    0,    0,    0,    0]]),\n",
       " tensor([8, 7, 7, 4, 3]),\n",
       " tensor([[6772,   70,  112,  167,  898],\n",
       "         [   4, 1119,   12,    4,    4],\n",
       "         [   2,  782,  201,    2, 5288],\n",
       "         [   0,   36, 2395,    0,    4],\n",
       "         [   0,   38,    4,    0,    2],\n",
       "         [   0,  234,    2,    0,    0],\n",
       "         [   0,  173,    0,    0,    0],\n",
       "         [   0,    4,    0,    0,    0],\n",
       "         [   0,    2,    0,    0,    0]]),\n",
       " tensor([[1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1],\n",
       "         [0, 1, 1, 0, 1],\n",
       "         [0, 1, 1, 0, 1],\n",
       "         [0, 1, 1, 0, 0],\n",
       "         [0, 1, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0]], dtype=torch.uint8),\n",
       " 9)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variable, length, target_variable, mask, max_length_len = batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  50,  124,   50, 1026,  318],\n",
       "        [  37,  519,   92,   62,    4],\n",
       "        [  53,   66,    7,    6,    2],\n",
       "        [ 341,   12,  937,    2,    0],\n",
       "        [ 169, 2468,  177,    0,    0],\n",
       "        [   7,   66,    6,    0,    0],\n",
       "        [   6,    2,    2,    0,    0],\n",
       "        [   2,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 7, 7, 4, 3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6772,   70,  112,  167,  898],\n",
       "        [   4, 1119,   12,    4,    4],\n",
       "        [   2,  782,  201,    2, 5288],\n",
       "        [   0,   36, 2395,    0,    4],\n",
       "        [   0,   38,    4,    0,    2],\n",
       "        [   0,  234,    2,    0,    0],\n",
       "        [   0,  173,    0,    0,    0],\n",
       "        [   0,    4,    0,    0,    0],\n",
       "        [   0,    2,    0,    0,    0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 0, 1],\n",
       "        [0, 1, 1, 0, 1],\n",
       "        [0, 1, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input_seqはembeddingする前なので2D tensor (max_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout),\n",
    "                          bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, 'is not an appropriate attention method.')\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "        \n",
    "    # encoder_outupt = (seq_len, batch, hidden_size)\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # 1単語ずつ入力する\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        \n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        \n",
    "        # 現在のRNNの出力とエンコーダーの出力を使う\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        \n",
    "        # エンコーダーの出力をattention重みで重みづけしてコンテキストベクトルとする\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        \n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        \n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = - torch.log(torch.gather(inp, 1, target.view(-1, 1)))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable,\n",
    "          mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip,\n",
    "          max_length=MAX_LENGTH):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    \n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # clip gradients\n",
    "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return sum(print_losses) / n_totals          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder,\n",
    "               encoder_optimizer, decoder_optimizer,\n",
    "               embedding, encoder_n_layers, decoder_n_layers,\n",
    "               save_dir, n_iteration, batch_size,\n",
    "               print_every, save_every, clip, corpus_name, loadFilename):\n",
    "    # 全iterationのバッチをまとめて生成（メモリ大丈夫？）\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)]\n",
    "    \n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "    \n",
    "    print('Training ...')\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        \n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "        \n",
    "        loss = train(input_variable, lengths, target_variable, mask,\n",
    "                     max_target_len, encoder, decoder, embedding,\n",
    "                     encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "        \n",
    "        # print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print('Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}'.format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "        \n",
    "        # save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name,\n",
    "                                     '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "\n",
    "# loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                             '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                             '{}_checkpoint.tar'.format(checkpoint_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadFilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadFilename:\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    \n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 7826個の単語を500次元の蜜ベクトルにEmbeddingする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n"
     ]
    }
   ],
   "source": [
    "print('Building encoder and decoder ...')\n",
    "\n",
    "# initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(7826, 500)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (embedding): Embedding(7826, 500)\n",
       "  (gru): GRU(500, 500, num_layers=2, dropout=0.1, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LuongAttnDecoderRNN(\n",
       "  (embedding): Embedding(7826, 500)\n",
       "  (embedding_dropout): Dropout(p=0.1)\n",
       "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
       "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (out): Linear(in_features=500, out_features=7826, bias=True)\n",
       "  (attn): Attn()\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size,\n",
    "                              voc.num_words, decoder_n_layers, dropout)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n"
     ]
    }
   ],
   "source": [
    "# configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cb_model\n",
      "<__main__.Voc object at 0x12a4776a0>\n",
      "[['there .', 'where ?'], ['you have my word . as a gentleman', 'you re sweet .'], ['hi .', 'looks like things worked out tonight huh ?']]\n",
      "EncoderRNN(\n",
      "  (embedding): Embedding(7826, 500)\n",
      "  (gru): GRU(500, 500, num_layers=2, dropout=0.1, bidirectional=True)\n",
      ")\n",
      "LuongAttnDecoderRNN(\n",
      "  (embedding): Embedding(7826, 500)\n",
      "  (embedding_dropout): Dropout(p=0.1)\n",
      "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
      "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
      "  (out): Linear(in_features=500, out_features=7826, bias=True)\n",
      "  (attn): Attn()\n",
      ")\n",
      "Embedding(7826, 500)\n",
      "2 2\n",
      "data/save\n",
      "4000\n",
      "1 500\n",
      "50.0\n",
      "cornell movie-dialogs corpus\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_name)\n",
    "print(voc)\n",
    "print(pairs[:3])\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "print(embedding)\n",
    "print(encoder_n_layers, decoder_n_layers)\n",
    "print(save_dir)\n",
    "print(n_iteration)\n",
    "print(print_every, save_every)\n",
    "print(clip)\n",
    "print(corpus_name)\n",
    "print(loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ...\n",
      "Training ...\n",
      "Iteration: 1; Percent complete: 0.0%; Average loss: 8.9574\n",
      "Iteration: 2; Percent complete: 0.1%; Average loss: 8.7525\n",
      "Iteration: 3; Percent complete: 0.1%; Average loss: 8.4271\n",
      "Iteration: 4; Percent complete: 0.1%; Average loss: 8.1515\n",
      "Iteration: 5; Percent complete: 0.1%; Average loss: 7.5345\n",
      "Iteration: 6; Percent complete: 0.1%; Average loss: 7.0011\n",
      "Iteration: 7; Percent complete: 0.2%; Average loss: 6.5914\n",
      "Iteration: 8; Percent complete: 0.2%; Average loss: 6.7082\n",
      "Iteration: 9; Percent complete: 0.2%; Average loss: 6.3685\n",
      "Iteration: 10; Percent complete: 0.2%; Average loss: 6.1353\n",
      "Iteration: 11; Percent complete: 0.3%; Average loss: 6.0705\n",
      "Iteration: 12; Percent complete: 0.3%; Average loss: 5.8791\n",
      "Iteration: 13; Percent complete: 0.3%; Average loss: 5.7090\n",
      "Iteration: 14; Percent complete: 0.4%; Average loss: 4.8764\n",
      "Iteration: 15; Percent complete: 0.4%; Average loss: 5.0021\n",
      "Iteration: 16; Percent complete: 0.4%; Average loss: 5.0650\n",
      "Iteration: 17; Percent complete: 0.4%; Average loss: 4.5459\n",
      "Iteration: 18; Percent complete: 0.4%; Average loss: 4.6667\n",
      "Iteration: 19; Percent complete: 0.5%; Average loss: 4.6159\n",
      "Iteration: 20; Percent complete: 0.5%; Average loss: 4.2855\n",
      "Iteration: 21; Percent complete: 0.5%; Average loss: 4.6019\n",
      "Iteration: 22; Percent complete: 0.5%; Average loss: 4.4574\n",
      "Iteration: 23; Percent complete: 0.6%; Average loss: 4.3064\n",
      "Iteration: 24; Percent complete: 0.6%; Average loss: 4.5042\n",
      "Iteration: 25; Percent complete: 0.6%; Average loss: 4.5268\n",
      "Iteration: 26; Percent complete: 0.7%; Average loss: 4.2260\n",
      "Iteration: 27; Percent complete: 0.7%; Average loss: 4.3933\n",
      "Iteration: 28; Percent complete: 0.7%; Average loss: 4.1535\n",
      "Iteration: 29; Percent complete: 0.7%; Average loss: 4.5106\n",
      "Iteration: 30; Percent complete: 0.8%; Average loss: 3.9014\n",
      "Iteration: 31; Percent complete: 0.8%; Average loss: 4.4785\n",
      "Iteration: 32; Percent complete: 0.8%; Average loss: 4.1397\n",
      "Iteration: 33; Percent complete: 0.8%; Average loss: 4.1558\n",
      "Iteration: 34; Percent complete: 0.9%; Average loss: 4.3095\n",
      "Iteration: 35; Percent complete: 0.9%; Average loss: 4.1940\n",
      "Iteration: 36; Percent complete: 0.9%; Average loss: 4.0997\n",
      "Iteration: 37; Percent complete: 0.9%; Average loss: 3.9913\n",
      "Iteration: 38; Percent complete: 0.9%; Average loss: 4.3100\n",
      "Iteration: 39; Percent complete: 1.0%; Average loss: 4.1215\n",
      "Iteration: 40; Percent complete: 1.0%; Average loss: 4.0720\n",
      "Iteration: 41; Percent complete: 1.0%; Average loss: 3.8264\n",
      "Iteration: 42; Percent complete: 1.1%; Average loss: 4.4272\n",
      "Iteration: 43; Percent complete: 1.1%; Average loss: 3.9249\n",
      "Iteration: 44; Percent complete: 1.1%; Average loss: 4.0286\n",
      "Iteration: 45; Percent complete: 1.1%; Average loss: 4.1515\n",
      "Iteration: 46; Percent complete: 1.1%; Average loss: 3.8066\n",
      "Iteration: 47; Percent complete: 1.2%; Average loss: 4.1470\n",
      "Iteration: 48; Percent complete: 1.2%; Average loss: 3.8187\n",
      "Iteration: 49; Percent complete: 1.2%; Average loss: 4.0372\n",
      "Iteration: 50; Percent complete: 1.2%; Average loss: 4.0826\n",
      "Iteration: 51; Percent complete: 1.3%; Average loss: 4.2104\n",
      "Iteration: 52; Percent complete: 1.3%; Average loss: 3.8697\n",
      "Iteration: 53; Percent complete: 1.3%; Average loss: 3.9354\n",
      "Iteration: 54; Percent complete: 1.4%; Average loss: 4.1944\n",
      "Iteration: 55; Percent complete: 1.4%; Average loss: 3.9926\n",
      "Iteration: 56; Percent complete: 1.4%; Average loss: 4.0023\n",
      "Iteration: 57; Percent complete: 1.4%; Average loss: 3.5830\n",
      "Iteration: 58; Percent complete: 1.5%; Average loss: 3.9117\n",
      "Iteration: 59; Percent complete: 1.5%; Average loss: 3.9073\n",
      "Iteration: 60; Percent complete: 1.5%; Average loss: 3.6851\n",
      "Iteration: 61; Percent complete: 1.5%; Average loss: 3.8817\n",
      "Iteration: 62; Percent complete: 1.6%; Average loss: 3.8632\n",
      "Iteration: 63; Percent complete: 1.6%; Average loss: 4.0391\n",
      "Iteration: 64; Percent complete: 1.6%; Average loss: 3.7949\n",
      "Iteration: 65; Percent complete: 1.6%; Average loss: 4.1185\n",
      "Iteration: 66; Percent complete: 1.7%; Average loss: 3.8767\n",
      "Iteration: 67; Percent complete: 1.7%; Average loss: 4.1009\n",
      "Iteration: 68; Percent complete: 1.7%; Average loss: 4.1117\n",
      "Iteration: 69; Percent complete: 1.7%; Average loss: 3.9322\n",
      "Iteration: 70; Percent complete: 1.8%; Average loss: 3.9863\n",
      "Iteration: 71; Percent complete: 1.8%; Average loss: 3.9826\n",
      "Iteration: 72; Percent complete: 1.8%; Average loss: 3.8452\n",
      "Iteration: 73; Percent complete: 1.8%; Average loss: 3.8702\n",
      "Iteration: 74; Percent complete: 1.8%; Average loss: 3.7743\n",
      "Iteration: 75; Percent complete: 1.9%; Average loss: 3.8001\n",
      "Iteration: 76; Percent complete: 1.9%; Average loss: 4.1303\n",
      "Iteration: 77; Percent complete: 1.9%; Average loss: 3.9033\n",
      "Iteration: 78; Percent complete: 1.9%; Average loss: 3.9928\n",
      "Iteration: 79; Percent complete: 2.0%; Average loss: 3.7828\n",
      "Iteration: 80; Percent complete: 2.0%; Average loss: 4.0448\n",
      "Iteration: 81; Percent complete: 2.0%; Average loss: 3.7939\n",
      "Iteration: 82; Percent complete: 2.1%; Average loss: 3.9064\n",
      "Iteration: 83; Percent complete: 2.1%; Average loss: 3.9236\n",
      "Iteration: 84; Percent complete: 2.1%; Average loss: 3.9738\n",
      "Iteration: 85; Percent complete: 2.1%; Average loss: 3.8444\n",
      "Iteration: 86; Percent complete: 2.1%; Average loss: 3.9249\n",
      "Iteration: 87; Percent complete: 2.2%; Average loss: 4.0273\n",
      "Iteration: 88; Percent complete: 2.2%; Average loss: 3.6308\n",
      "Iteration: 89; Percent complete: 2.2%; Average loss: 3.6249\n",
      "Iteration: 90; Percent complete: 2.2%; Average loss: 4.0964\n",
      "Iteration: 91; Percent complete: 2.3%; Average loss: 3.9145\n",
      "Iteration: 92; Percent complete: 2.3%; Average loss: 3.8361\n",
      "Iteration: 93; Percent complete: 2.3%; Average loss: 3.6970\n",
      "Iteration: 94; Percent complete: 2.4%; Average loss: 3.7000\n",
      "Iteration: 95; Percent complete: 2.4%; Average loss: 3.8991\n",
      "Iteration: 96; Percent complete: 2.4%; Average loss: 3.9177\n",
      "Iteration: 97; Percent complete: 2.4%; Average loss: 3.8434\n",
      "Iteration: 98; Percent complete: 2.5%; Average loss: 3.8746\n",
      "Iteration: 99; Percent complete: 2.5%; Average loss: 4.0598\n",
      "Iteration: 100; Percent complete: 2.5%; Average loss: 3.9584\n",
      "Iteration: 101; Percent complete: 2.5%; Average loss: 3.6906\n",
      "Iteration: 102; Percent complete: 2.5%; Average loss: 3.7749\n",
      "Iteration: 103; Percent complete: 2.6%; Average loss: 3.8629\n",
      "Iteration: 104; Percent complete: 2.6%; Average loss: 3.8437\n",
      "Iteration: 105; Percent complete: 2.6%; Average loss: 3.9572\n",
      "Iteration: 106; Percent complete: 2.6%; Average loss: 3.8730\n",
      "Iteration: 107; Percent complete: 2.7%; Average loss: 3.8414\n",
      "Iteration: 108; Percent complete: 2.7%; Average loss: 3.9331\n",
      "Iteration: 109; Percent complete: 2.7%; Average loss: 3.7640\n",
      "Iteration: 110; Percent complete: 2.8%; Average loss: 3.7579\n",
      "Iteration: 111; Percent complete: 2.8%; Average loss: 3.8373\n",
      "Iteration: 112; Percent complete: 2.8%; Average loss: 3.8423\n",
      "Iteration: 113; Percent complete: 2.8%; Average loss: 3.5009\n",
      "Iteration: 114; Percent complete: 2.9%; Average loss: 3.8246\n",
      "Iteration: 115; Percent complete: 2.9%; Average loss: 3.5574\n",
      "Iteration: 116; Percent complete: 2.9%; Average loss: 3.7170\n",
      "Iteration: 117; Percent complete: 2.9%; Average loss: 3.9506\n",
      "Iteration: 118; Percent complete: 2.9%; Average loss: 3.7315\n",
      "Iteration: 119; Percent complete: 3.0%; Average loss: 3.6310\n",
      "Iteration: 120; Percent complete: 3.0%; Average loss: 3.8062\n",
      "Iteration: 121; Percent complete: 3.0%; Average loss: 3.8378\n",
      "Iteration: 122; Percent complete: 3.0%; Average loss: 3.7788\n",
      "Iteration: 123; Percent complete: 3.1%; Average loss: 3.7665\n",
      "Iteration: 124; Percent complete: 3.1%; Average loss: 3.7139\n",
      "Iteration: 125; Percent complete: 3.1%; Average loss: 3.5640\n",
      "Iteration: 126; Percent complete: 3.1%; Average loss: 3.8932\n",
      "Iteration: 127; Percent complete: 3.2%; Average loss: 3.6817\n",
      "Iteration: 128; Percent complete: 3.2%; Average loss: 3.9082\n",
      "Iteration: 129; Percent complete: 3.2%; Average loss: 3.6364\n",
      "Iteration: 130; Percent complete: 3.2%; Average loss: 3.8153\n",
      "Iteration: 131; Percent complete: 3.3%; Average loss: 3.9974\n",
      "Iteration: 132; Percent complete: 3.3%; Average loss: 3.6497\n",
      "Iteration: 133; Percent complete: 3.3%; Average loss: 3.5018\n",
      "Iteration: 134; Percent complete: 3.4%; Average loss: 3.8085\n",
      "Iteration: 135; Percent complete: 3.4%; Average loss: 3.5122\n",
      "Iteration: 136; Percent complete: 3.4%; Average loss: 3.8044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-ad041817b3fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m            \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m            \u001b[0mprint_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m            corpus_name, loadFilename)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-123-f8cac4d8e9d2>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename)\u001b[0m\n\u001b[1;32m     21\u001b[0m         loss = train(input_variable, lengths, target_variable, mask,\n\u001b[1;32m     22\u001b[0m                      \u001b[0mmax_target_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                      encoder_optimizer, decoder_optimizer, batch_size, clip)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-ccf7252b6573>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mn_totals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnTotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# clip gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainIters(model_name, voc, pairs,\n",
    "           encoder, decoder,\n",
    "           encoder_optimizer, decoder_optimizer,\n",
    "           embedding,\n",
    "           encoder_n_layers, decoder_n_layers,\n",
    "           save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip,\n",
    "           corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
